{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e3efe2cf-9584-4842-ae2e-9029515fb751",
   "metadata": {},
   "source": [
    "# **Reinforcement Learning algorithm**\n",
    "- Author: Kim Dohwan [@ehghks021203](https://github.com/ehghks021203)\r\n",
    "- Date: 2043.39210. ~\r\n",
    "- DescriptionAutonomous self-driving car using DQN algorithm in CARLA environmen\n",
    "\n",
    "---\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d61a1e2-b709-4335-8fac-809b6f097d3b",
   "metadata": {},
   "source": [
    "## **Load libraries**\n",
    "Import carla module and PyTorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ee1cde4a-22d5-4844-9ddd-b381c3ee2bff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pygame 2.5.2 (SDL 2.28.3, Python 3.8.1)\n",
      "Hello from the pygame community. https://www.pygame.org/contribute.html\n"
     ]
    }
   ],
   "source": [
    "# Add file path\n",
    "import os\n",
    "import sys\n",
    "sys.path.append(os.path.dirname(os.path.abspath('')))\n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "from collections import namedtuple, deque\n",
    "import cv2\n",
    "import math\n",
    "import copy\n",
    "import pygame\n",
    "\n",
    "# Import CARLA modules\n",
    "import carla\n",
    "from src.agents.navigation.controller import PIDLongitudinalController\n",
    "from src.agents.navigation.global_route_planner import GlobalRoutePlanner\n",
    "from src.synchronous_mode import CarlaSyncMode\n",
    "\n",
    "# Import PyTorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0410f20-a7c2-4bc1-9e22-8a24e470e32e",
   "metadata": {},
   "source": [
    "## **Setting parameter(map, vehicle, etc)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "feb7ad1b-a193-4ff6-aa57-0f692d9e5e44",
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    \"map\": \"Town04_Opt\",\n",
    "    \"vehicle\": \"vehicle.mini.cooper_s_2021\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60722d55-5d6b-4c85-929c-8867065e2579",
   "metadata": {},
   "source": [
    "## **Make carla environment**\n",
    "\n",
    "CARLA 환경에 관한 클래스를 선언합니다.\n",
    "\n",
    "CARLA 환경은 크게 4단계로 구성되어 있습니다.\n",
    "\n",
    "### **Key Steps:**\n",
    "#### **1. __init__()**\n",
    "- CARLA 환경을 초기화합니다. 여기에는 CARLA Client 연결, 맵 로드, Blueprint library 받아오기 등의 작업이 있습니다.\n",
    "\n",
    "#### **2. reset()**\n",
    "- CARLA 환경의 상태를 모두 초기화합니다.\n",
    "\n",
    "#### **3. step()**\n",
    "- "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "941ddcd8-fc98-427f-9055-4ae699ace543",
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_image(surface, image, blend=False):\n",
    "    array = np.frombuffer(image.raw_data, dtype=np.dtype(\"uint8\"))\n",
    "    array = np.reshape(array, (image.height, image.width, 4))\n",
    "    array = array[:, :, :3]\n",
    "    array = array[:, :, ::-1]\n",
    "    image_surface = pygame.surfarray.make_surface(array.swapaxes(0, 1))\n",
    "    if blend:\n",
    "        image_surface.set_alpha(100)\n",
    "    surface.blit(image_surface, (0, 0))\n",
    "\n",
    "def get_font():\n",
    "    fonts = [x for x in pygame.font.get_fonts()]\n",
    "    default_font = 'ubuntumono'\n",
    "    font = default_font if default_font in fonts else fonts[0]\n",
    "    font = pygame.font.match_font(font)\n",
    "    return pygame.font.Font(font, 14)\n",
    "\n",
    "def should_quit():\n",
    "    for event in pygame.event.get():\n",
    "        if event.type == pygame.QUIT:\n",
    "            return True\n",
    "        elif event.type == pygame.KEYUP:\n",
    "            if event.key == pygame.K_ESCAPE:\n",
    "                return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "74d9bbbe-abda-49b7-b138-c2a6313a3e95",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CarEnv():\n",
    "    def __init__(\n",
    "        self, \n",
    "        visuals=True,\n",
    "        target_speed = 30,\n",
    "        max_iter = 4000,\n",
    "        start_buffer = 10,\n",
    "        train_freq = 1,\n",
    "        save_freq = 10,\n",
    "        start_ep = 0,\n",
    "        max_dist_from_waypoint = 20\n",
    "    ) -> None:\n",
    "        if visuals:\n",
    "            self.pg_display = PyGameDisplay()\n",
    "        \n",
    "        # Connect CARLA client\n",
    "        self.client = carla.Client(\"localhost\", 2000)\n",
    "        self.client.set_timeout(10)\n",
    "\n",
    "        # Get world data\n",
    "        self.world = self.client.load_world(params[\"map\"])\n",
    "        self.world.unload_map_layer(carla.MapLayer.Decals)\n",
    "        self.world.unload_map_layer(carla.MapLayer.Foliage)\n",
    "        self.world.unload_map_layer(carla.MapLayer.ParkedVehicles)\n",
    "        self.world.unload_map_layer(carla.MapLayer.Particles)\n",
    "        self.world.unload_map_layer(carla.MapLayer.Props)\n",
    "        self.world.unload_map_layer(carla.MapLayer.StreetLights)\n",
    "\n",
    "        # Get map from world\n",
    "        self.map = self.world.get_map()\n",
    "\n",
    "        # Get blueprint library\n",
    "        self.blueprint_library = self.world.get_blueprint_library()\n",
    "\n",
    "        # reset actor list\n",
    "        self.vehicle_list = []\n",
    "\n",
    "        # Environment Init\n",
    "        self.reset()\n",
    "        \n",
    "        # input these later on as arguments\n",
    "        self.visuals = visuals\n",
    "        self.global_t = 0 # global timestep\n",
    "        self.target_speed = target_speed # km/h \n",
    "        self.max_iter = max_iter\n",
    "        self.start_buffer = start_buffer\n",
    "        self.train_freq = train_freq\n",
    "        self.save_freq = save_freq\n",
    "        self.start_ep = start_ep\n",
    "\n",
    "        self.max_dist_from_waypoint = max_dist_from_waypoint\n",
    "        self.start_train = self.start_ep + self.start_buffer\n",
    "        \n",
    "        self.total_rewards = 0\n",
    "        self.average_rewards_list = []\n",
    "\n",
    "\n",
    "    def reset(self):\n",
    "        # reset actor list\n",
    "        for vehicle in self.vehicle_list:\n",
    "            vehicle.destroy()\n",
    "\n",
    "        self.vehicle_list = []\n",
    "        \n",
    "        # Create Waypoints\n",
    "        self.waypoint = Waypoint(self)\n",
    "\n",
    "        # Create vehicle actor\n",
    "        self.vehicle = Vehicle(self, self.waypoint)\n",
    "        self.vehicle_list.append(self.vehicle)\n",
    "\n",
    "        # Reset global timestep\n",
    "        self.g_time = 0\n",
    "\n",
    "    \n",
    "    def step(self, model, replay_memory, ep, action_map, eval=False):\n",
    "        with CarlaSyncMode(self.world, self.vehicle.rgb_cam, self.vehicle.observe_cam, self.vehicle.collision_sensor, fps=30) as sync_mode:\n",
    "            # Initialize agent step count\n",
    "            step_count = 0\n",
    "\n",
    "            # Get state from world\n",
    "            snapshot, image_rgb, image_observe, collision = sync_mode.tick(timeout=2.0)\n",
    "            \n",
    "            # destroy if there is no data\n",
    "            if snapshot is None or image_rgb is None:\n",
    "                print(\"No data, skipping episode\")\n",
    "                self.reset()\n",
    "                return None\n",
    "\n",
    "            image = self.vehicle.process_img(image_rgb)\n",
    "            next_state = image \n",
    "\n",
    "            while True:\n",
    "                # Get vehicle state\n",
    "                vehicle_location = self.vehicle.get_location()\n",
    "                waypoint = self.world.get_map().get_waypoint(vehicle_location, project_to_road=True, lane_type=carla.LaneType.Driving)\n",
    "                speed = self.vehicle.get_speed()\n",
    "\n",
    "                # Render waypoint every 10 step count\n",
    "                if step_count % 10 == 0:\n",
    "                    #self.waypoint.render()\n",
    "                    self.world.debug.draw_string(\n",
    "                        waypoint.transform.location, \n",
    "                        'O', \n",
    "                        draw_shadow=False,\n",
    "                        color=carla.Color(r=255, g=0, b=0), \n",
    "                        life_time=0.2,\n",
    "                        persistent_lines=True\n",
    "                )\n",
    "\n",
    "                # Advance the simulation and wait for the data.\n",
    "                state = next_state\n",
    "\n",
    "                step_count += 1\n",
    "                self.g_time += 1\n",
    "\n",
    "                action = model.select_action(state, eval=eval)\n",
    "                steer = action\n",
    "                if action_map is not None:\n",
    "                    steer = action_map[action]\n",
    "\n",
    "                # Select action based on state(rgb camera)\n",
    "                self.vehicle.apply_control(steer)\n",
    "\n",
    "                fps = round(1.0 / snapshot.timestamp.delta_seconds)\n",
    "\n",
    "                snapshot, image_rgb, image_observe, collision = sync_mode.tick(timeout=2.0)\n",
    "\n",
    "                reward, collision = get_reward_comp(self.vehicle, waypoint, collision)\n",
    "\n",
    "                if snapshot is None or image_rgb is None:\n",
    "                    print(\"Process ended here\")\n",
    "                    break\n",
    "\n",
    "                image = self.vehicle.process_img(image_rgb)\n",
    "\n",
    "                done = 1 if collision else 0\n",
    "\n",
    "                self.total_rewards += reward\n",
    "\n",
    "                next_state = image\n",
    "\n",
    "                replay_memory.add(state, action, next_state, reward, done)\n",
    "\n",
    "                if not eval:\n",
    "                    if ep > self.start_train and (self.g_time % self.train_freq) == 0:\n",
    "                        model.train(replay_memory)\n",
    "\n",
    "                # Draw the display.\n",
    "                # Draw the display.\n",
    "                if self.visuals:\n",
    "                    self.render(image_observe, fps)\n",
    "\n",
    "                if collision == 1 or step_count >= 1000:\n",
    "                    print(\"Episode {} processed\".format(ep), step_count)\n",
    "                    break\n",
    "            \n",
    "            if ep % self.save_freq == 0 and ep > 0:\n",
    "                self.save(model, ep)\n",
    "\n",
    "    \n",
    "    def render(self, image_observe, fps):\n",
    "        draw_image(self.pg_display.display, image_observe)\n",
    "        self.pg_display.display.blit(\n",
    "            self.pg_display.font.render('% 5d FPS (real)' % self.pg_display.clock.get_fps(), True, (255, 255, 255)),\n",
    "            (8, 10)\n",
    "        )\n",
    "        self.pg_display.display.blit(\n",
    "            self.pg_display.font.render('% 5d FPS (simulated)' % fps, True, (255, 255, 255)),\n",
    "            (8, 28)\n",
    "        )\n",
    "        pygame.display.flip()\n",
    "\n",
    "    def save(self, model, ep):\n",
    "        if ep % self.save_freq == 0 and ep > self.start_ep:\n",
    "            avg_reward = self.total_rewards/self.save_freq\n",
    "            self.average_rewards_list.append(avg_reward)\n",
    "            self.total_rewards = 0\n",
    "\n",
    "            model.save(\"../weights/model_ep_{}\".format(ep))\n",
    "\n",
    "            print(\"Saved model with average reward =\", avg_reward)\n",
    "    \n",
    "    def close(self):\n",
    "        pygame.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dfeb8c7e-a7f6-4f0d-bca2-79b7b0b40950",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vehicle():\n",
    "    def __init__(self, env, waypoint):\n",
    "        # Initialize actor list\n",
    "        self.actor_list = []\n",
    "        \n",
    "        # Set vehicle spawn point\n",
    "        transform = waypoint.start_point\n",
    "        self.vehicle_model = env.blueprint_library.filter(params[\"vehicle\"])[0]\n",
    "\n",
    "        # Spawning the vehicle\n",
    "        self.vehicle = env.world.spawn_actor(self.vehicle_model, transform)\n",
    "        self.actor_list.append(self.vehicle)\n",
    "        \n",
    "        # Attach Camera with Vehicle\n",
    "        self.rgb_cam = env.world.spawn_actor(\n",
    "            env.blueprint_library.find('sensor.camera.rgb'),\n",
    "            carla.Transform(carla.Location(x=1.5, z=2.4), carla.Rotation(pitch=-15)),\n",
    "            attach_to=self.vehicle)\n",
    "        self.actor_list.append(self.rgb_cam)\n",
    "\n",
    "        # Observer Camera\n",
    "        self.observe_cam = env.world.spawn_actor(\n",
    "            env.blueprint_library.find('sensor.camera.rgb'),\n",
    "            carla.Transform(carla.Location(x=-1.5, z=17.8), carla.Rotation(pitch=-90)),\n",
    "            attach_to=self.vehicle)\n",
    "        self.actor_list.append(self.observe_cam)\n",
    "        \n",
    "        # Attach Collision Sensor with Vehicle\n",
    "        self.collision_sensor = env.world.spawn_actor(\n",
    "            env.blueprint_library.find('sensor.other.collision'),\n",
    "            carla.Transform(),\n",
    "            attach_to=self.vehicle\n",
    "        )\n",
    "        self.actor_list.append(self.collision_sensor)\n",
    "\n",
    "        self.speed_controller = PIDLongitudinalController(self.vehicle)\n",
    "\n",
    "        self.target_speed = 30\n",
    "\n",
    "    def get_location(self):\n",
    "        return self.vehicle.get_location()\n",
    "\n",
    "    def get_transform(self):\n",
    "        return self.vehicle.get_transform()\n",
    "\n",
    "    def get_speed(self):\n",
    "        \"\"\"\n",
    "        Compute speed of a vehicle in Km/h.\n",
    "            :param vehicle: the vehicle for which speed is calculated\n",
    "            :return: speed as a float in Km/h\n",
    "        \"\"\"\n",
    "        vel = self.vehicle.get_velocity()\n",
    "    \n",
    "        return 3.6 * math.sqrt(vel.x ** 2 + vel.y ** 2 + vel.z ** 2)\n",
    "\n",
    "    def apply_control(self, steer):\n",
    "        control = self.speed_controller.run_step(self.target_speed)\n",
    "        control.steer = steer\n",
    "        self.vehicle.apply_control(control)\n",
    "\n",
    "    def process_img(self, image, dim_x=128, dim_y=128):\n",
    "        array = np.frombuffer(image.raw_data, dtype=np.dtype(\"uint8\"))\n",
    "        array = np.reshape(array, (image.height, image.width, 4))\n",
    "        array = array[:, :, :3]\n",
    "        array = array[:, :, ::-1]\n",
    "    \n",
    "        dim = (dim_x, dim_y)  # set same dim for now\n",
    "        resized_img = cv2.resize(array, dim, interpolation=cv2.INTER_AREA)\n",
    "        img_gray = cv2.cvtColor(resized_img, cv2.COLOR_BGR2GRAY)\n",
    "        scaledImg = img_gray/255.\n",
    "    \n",
    "        # normalize\n",
    "        mean, std = 0.5, 0.5\n",
    "        normalizedImg = (scaledImg - mean) / std\n",
    "    \n",
    "        return normalizedImg\n",
    "\n",
    "    def collision_data(self, event):\n",
    "        lane_types = set(x.type for x in event.crossed_lane_markings)\n",
    "        text = ['%r' % str(x).split()[-1] for x in lane_types]\n",
    "        print('Crossed line %s' % ' and '.join(text))\n",
    "\n",
    "    def destroy(self):\n",
    "        for actor in self.actor_list:\n",
    "            actor.destroy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f5e514ff-930b-4758-89ff-391387148756",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Waypoint():\n",
    "    def __init__(self, env):\n",
    "        self.env = env\n",
    "        self.sampling_resolution = 2\n",
    "        self.grp = GlobalRoutePlanner(env.map, self.sampling_resolution)\n",
    "        spawn_points = env.map.get_spawn_points()\n",
    "        \n",
    "        # self.start_point = random.choice(env.map.get_spawn_points())\n",
    "        # self.end_point = random.choice(env.map.get_spawn_points())\n",
    "        # self.start_point_location = carla.Location(self.start_point.location)\n",
    "        # self.end_point_location = carla.Location(self.end_point.location)\n",
    "        # self.waypoints = self.grp.trace_route(self.start_point_location, self.end_point_location) # there are other funcations can be used to generate a route in GlobalRoutePlanner.\n",
    "        \n",
    "        self.start_point = spawn_points[100]\n",
    "        self.end_point = spawn_points[100]\n",
    "        self.start_point_location = carla.Location(self.start_point.location)\n",
    "        self.end_point_location = carla.Location(self.end_point.location)\n",
    "        self.waypoints = self.grp.trace_route(self.start_point_location, self.end_point_location)\n",
    "\n",
    "    def render(self):\n",
    "        i = 0\n",
    "        for w in self.waypoints:\n",
    "            if i % 10 == 0:\n",
    "                self.env.world.debug.draw_string(w[0].transform.location, 'O', draw_shadow=False,\n",
    "                color=carla.Color(r=255, g=0, b=0), life_time=0.2,\n",
    "                persistent_lines=True)\n",
    "            else:\n",
    "                self.env.world.debug.draw_string(w[0].transform.location, 'O', draw_shadow=False,\n",
    "                color = carla.Color(r=0, g=0, b=255), life_time=0.2,\n",
    "                persistent_lines=True)\n",
    "            i += 1\n",
    "\n",
    "    def get_next_waypoint(self, vehicle):\n",
    "        vehicle_location = vehicle.get_location()\n",
    "        min_distance = 1000\n",
    "        next_waypoint = None\n",
    "\n",
    "        for waypoint in self.waypoints:\n",
    "            waypoint_location = waypoint.transform.location\n",
    "\n",
    "            #Only check waypoints that are in the front of the vehicle (if x is negative, then the waypoint is to the rear)\n",
    "            #TODO: Check if this applies for all maps\n",
    "            if (waypoint_location - vehicle_location).x > 0:\n",
    "    \n",
    "                #Find the waypoint closest to the vehicle, but once vehicle is close to upcoming waypoint, search for next one\n",
    "                if vehicle_location.distance(waypoint_location) < min_distance and vehicle_location.distance(waypoint_location) > 5:\n",
    "                    min_distance = vehicle_location.distance(waypoint_location)\n",
    "                    next_waypoint = waypoint\n",
    "    \n",
    "        return next_waypoint"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f096cf7c-6106-44f9-8de0-2eba8bce1248",
   "metadata": {},
   "source": [
    "## Create ReplayMemory Class\n",
    "\n",
    "### 재현 메모리(Replay Memory)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "67587800-00e3-4414-8f13-8dc80497e853",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayMemory(object):\n",
    "    def __init__(self, state_dim, batch_size, buffer_size, device):\n",
    "        self.batch_size = batch_size\n",
    "        self.max_size = int(buffer_size)\n",
    "        self.device = device\n",
    "\n",
    "        self.ptr = 0\n",
    "        self.crt_size = 0\n",
    "\n",
    "        self.state = np.zeros((self.max_size,) + state_dim)\n",
    "        self.action = np.zeros((self.max_size, 1))\n",
    "        self.next_state = np.array(self.state)\n",
    "        self.reward = np.zeros((self.max_size, 1))\n",
    "        self.done = np.zeros((self.max_size, 1))\n",
    "\n",
    "    def add(self, state, action, next_state, reward, done):\n",
    "        self.state[self.ptr] = state\n",
    "        self.action[self.ptr] = action\n",
    "        self.next_state[self.ptr] = next_state\n",
    "        self.reward[self.ptr] = reward\n",
    "        self.done[self.ptr] = done\n",
    "\n",
    "        self.ptr = (self.ptr + 1) % self.max_size\n",
    "        self.crt_size = min(self.crt_size + 1, self.max_size)\n",
    "\n",
    "    def sample(self):\n",
    "        ind = np.random.randint(0, self.crt_size, size=self.batch_size)\n",
    "        return (\n",
    "            torch.FloatTensor(self.state[ind]).unsqueeze(1).to(self.device),\n",
    "            torch.FloatTensor(self.action[ind]).to(self.device),\n",
    "            torch.FloatTensor(self.next_state[ind]).unsqueeze(1).to(self.device),\n",
    "            torch.FloatTensor(self.reward[ind]).to(self.device),\n",
    "            torch.FloatTensor(self.done[ind]).to(self.device)\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "640b6354-ba1f-42d0-b5b6-2a314385294e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvNet(nn.Module):\n",
    "    def __init__(self, dim, in_channels, num_actions) -> None:\n",
    "        super(ConvNet, self).__init__()\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(in_channels, 32, 8, 4)\n",
    "        self.conv1_bn = nn.BatchNorm2d(32)\n",
    "        self.conv2 = nn.Conv2d(32, 64, 4, 3)\n",
    "        self.conv2_bn = nn.BatchNorm2d(64)\n",
    "        self.conv3 = nn.Conv2d(64, 64, 3, 1)\n",
    "        self.conv3_bn = nn.BatchNorm2d(64)\n",
    "\n",
    "        self.fc1 = nn.Linear(64*8*8, 256)\n",
    "        self.fc1_bn = nn.BatchNorm1d(256)\n",
    "        self.fc2 = nn.Linear(256, 32)\n",
    "        self.fc2_bn = nn.BatchNorm1d(32)\n",
    "        self.fc3 = nn.Linear(32, num_actions)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1_bn(self.conv1(x)))\n",
    "        x = F.relu(self.conv2_bn(self.conv2(x)))\n",
    "        x = F.relu(self.conv3_bn(self.conv3(x)))\n",
    "        x = F.relu(self.fc1_bn(self.fc1(x.reshape(-1, 64*8*8))))\n",
    "        x = F.relu(self.fc2_bn(self.fc2(x)))\n",
    "        x = self.fc3(x)\n",
    "        x = self.sigmoid(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "05d851a9-6df9-4f9d-bf04-14fc0e3a94f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQNAgent(object):\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_actions,\n",
    "        state_dim, #?\n",
    "        in_channels,\n",
    "        device,\n",
    "        discount=0.9,\n",
    "        optimizer=\"Adam\",\n",
    "        optimizer_parameters={'lr':0.01},\n",
    "        target_update_frequency=1e4,\n",
    "        initial_eps = 1,\n",
    "        end_eps = 0.05,\n",
    "        eps_decay_period = 25e4,\n",
    "        eval_eps=0.001\n",
    "    ) -> None:\n",
    "        # Set Device\n",
    "        self.device = device\n",
    "\n",
    "        self.Q = ConvNet(state_dim, in_channels, num_actions).to(self.device)\n",
    "        self.Q_target = copy.deepcopy(self.Q)  # copy target network\n",
    "        self.Q_optimizer = getattr(torch.optim, optimizer)(self.Q.parameters(), \n",
    "        **optimizer_parameters)\n",
    "\n",
    "        self.discount = discount\n",
    "\n",
    "        self.target_update_frequency = target_update_frequency\n",
    " \n",
    "        # epsilon decay\n",
    "        self.initial_eps = initial_eps\n",
    "        self.end_eps = end_eps\n",
    "        self.slope = (self.end_eps - self.initial_eps) / eps_decay_period\n",
    "\n",
    "        self.state_shape = (-1,) + state_dim\n",
    "        self.eval_eps = eval_eps\n",
    "        self.num_actions = num_actions\n",
    "\n",
    "        self.iterations = 0\n",
    "\n",
    "    def select_action(self, state, eval=False):\n",
    "        eps = self.eval_eps if eval else max(self.slope * self.iterations + self.initial_eps, self.end_eps)\n",
    "        self.current_eps = eps\n",
    "\n",
    "        # Select action according to policy with probability (1-eps)\n",
    "        # otherwise, select random action\n",
    "        if np.random.uniform(0,1) > eps:\n",
    "            self.Q.eval()\n",
    "            with torch.no_grad():\n",
    "                # without batch norm, remove the unsqueeze\n",
    "                state = torch.FloatTensor(state).reshape(self.state_shape).unsqueeze(0).to(self.device)\n",
    "                return int(self.Q(state).argmax(1))\n",
    "        else:\n",
    "            return np.random.randint(self.num_actions)\n",
    "\n",
    "\n",
    "    def train(self, replay_memory):\n",
    "        self.Q.train()\n",
    "        # Sample mininbatch from replay buffer\n",
    "        state, action, next_state, reward, done = replay_memory.sample()\n",
    "\n",
    "        # Convert action tensor to int64 data type\n",
    "        action = action.clone().detach().long()\n",
    "\n",
    "        # Compute the target Q value\n",
    "        with torch.no_grad():\n",
    "            target_Q = reward + (1-done) * self.discount * self.Q_target(next_state).max(1, keepdim=True)[0]\n",
    "\n",
    "        # Get current Q estimate\n",
    "        # torch gather just selects action values from Q(state) using the action tensor as an index\n",
    "        current_Q = self.Q(state).gather(1, action)\n",
    "\n",
    "        # Compute Q loss\n",
    "        Q_loss = F.smooth_l1_loss(current_Q, target_Q)\n",
    "\n",
    "        # Optimize the Q\n",
    "        self.Q_optimizer.zero_grad()\n",
    "        Q_loss.backward()\n",
    "        self.Q_optimizer.step()\n",
    "\n",
    "        # Update target network by full copy every X iterations.\n",
    "        self.iterations += 1\n",
    "        self.copy_target_update()\n",
    "    \n",
    "    def copy_target_update(self):\n",
    "        if self.iterations % self.target_update_frequency == 0:\n",
    "            print('target network updated')\n",
    "            print('current epsilon', self.current_eps)\n",
    "            self.Q_target.load_state_dict(self.Q.state_dict())\n",
    "\n",
    "\n",
    "    def save(self, filename):\n",
    "        torch.save(self.Q.state_dict(), filename + \"_Q\")\n",
    "        torch.save(self.Q_optimizer.state_dict(), filename + \"_optimizer\")\n",
    "\n",
    "\n",
    "    def load(self, filename):\n",
    "        self.Q.load_state_dict(torch.load(filename + \"_Q\"))\n",
    "        self.Q_target = copy.deepcopy(self.Q)\n",
    "        self.Q_optimizer.load_state_dict(torch.load(filename + \"_optimizer\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e8035236-5302-418c-a612-69b7cec4a57d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_reward_comp(vehicle, waypoint, collision):\n",
    "    vehicle_location = vehicle.get_location()\n",
    "    x_wp = waypoint.transform.location.x\n",
    "    y_wp = waypoint.transform.location.y\n",
    "\n",
    "    x_vh = vehicle_location.x\n",
    "    y_vh = vehicle_location.y\n",
    "\n",
    "    wp_array = np.array([x_wp, y_wp])\n",
    "    vh_array = np.array([x_vh, y_vh])\n",
    "\n",
    "    dist = np.linalg.norm(wp_array - vh_array)\n",
    "\n",
    "    vh_yaw = correct_yaw(vehicle.get_transform().rotation.yaw)\n",
    "    wp_yaw = correct_yaw(waypoint.transform.rotation.yaw)\n",
    "    cos_yaw_diff = np.cos((vh_yaw - wp_yaw)*np.pi/180.)\n",
    "\n",
    "    collision = 0 if collision is None else 1\n",
    "\n",
    "    speed = -1 if vehicle.get_speed() <= 20 else 1\n",
    "\n",
    "    reward = cos_yaw_diff - dist - (10 * collision) + (5 * speed)\n",
    "    \n",
    "    return reward, collision\n",
    "\n",
    "\n",
    "\n",
    "def correct_yaw(x):\n",
    "    return(((x%360) + 360) % 360)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7c125a0b-9339-4f7e-bf7d-5968c8a01638",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PyGameDisplay():\n",
    "    def __init__(self):\n",
    "        pygame.init()\n",
    "\n",
    "        self.display = pygame.display.set_mode(\n",
    "            (800, 600),\n",
    "            pygame.HWSURFACE | pygame.DOUBLEBUF)\n",
    "        self.font = self.get_font()\n",
    "        self.clock = pygame.time.Clock()\n",
    "\n",
    "    def get_font(self):\n",
    "        fonts = [x for x in pygame.font.get_fonts()]\n",
    "        default_font = 'ubuntumono'\n",
    "        font = default_font if default_font in fonts else fonts[0]\n",
    "        font = pygame.font.match_font(font)\n",
    "        return pygame.font.Font(font, 14)\n",
    "    \n",
    "    def should_quit(self):\n",
    "        for event in pygame.event.get():\n",
    "            if event.type == pygame.QUIT:\n",
    "                return True\n",
    "            elif event.type == pygame.KEYUP:\n",
    "                if event.key == pygame.K_ESCAPE:\n",
    "                    return True\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a2879765-353c-44e8-9baa-57cc38b4ad54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dqn action values\n",
    "action_values = [\n",
    "    -0.75, \n",
    "    -0.5, \n",
    "    -0.25, \n",
    "    -0.15, \n",
    "    -0.1, \n",
    "    -0.05, \n",
    "    0,\n",
    "    0.05, \n",
    "    0.1, \n",
    "    0.15, \n",
    "    0.25, \n",
    "    0.5, \n",
    "    0.75\n",
    "]\n",
    "action_map = {i:x for i, x in enumerate(action_values)}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ffa7ef34-fbeb-4a05-9744-e076422e8ba6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 0 processed 167\n",
      "Episode 1 processed 217\n",
      "Episode 2 processed 484\n",
      "Episode 3 processed 645\n",
      "Episode 4 processed 594\n",
      "Episode 5 processed 376\n",
      "Episode 6 processed 176\n",
      "Episode 7 processed 243\n",
      "Episode 8 processed 275\n",
      "Episode 9 processed 606\n",
      "Episode 10 processed 306\n",
      "Saved model with average reward = 1315.850896945139\n",
      "Episode 11 processed 413\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 21\u001b[0m\n\u001b[0;32m     18\u001b[0m     env \u001b[38;5;241m=\u001b[39m CarEnv(visuals\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m     20\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m ep \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(episodes):\n\u001b[1;32m---> 21\u001b[0m         \u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreplay_memory\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mep\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maction_map\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     22\u001b[0m         env\u001b[38;5;241m.\u001b[39mreset()\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n",
      "Cell \u001b[1;32mIn[4], line 135\u001b[0m, in \u001b[0;36mCarEnv.step\u001b[1;34m(self, model, replay_memory, ep, action_map, eval)\u001b[0m\n\u001b[0;32m    132\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mProcess ended here\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    133\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m--> 135\u001b[0m image \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvehicle\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprocess_img\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage_rgb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    137\u001b[0m done \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m collision \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m    139\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtotal_rewards \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m reward\n",
      "Cell \u001b[1;32mIn[5], line 68\u001b[0m, in \u001b[0;36mVehicle.process_img\u001b[1;34m(self, image, dim_x, dim_y)\u001b[0m\n\u001b[0;32m     65\u001b[0m array \u001b[38;5;241m=\u001b[39m array[:, :, ::\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m     67\u001b[0m dim \u001b[38;5;241m=\u001b[39m (dim_x, dim_y)  \u001b[38;5;66;03m# set same dim for now\u001b[39;00m\n\u001b[1;32m---> 68\u001b[0m resized_img \u001b[38;5;241m=\u001b[39m \u001b[43mcv2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresize\u001b[49m\u001b[43m(\u001b[49m\u001b[43marray\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minterpolation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcv2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mINTER_AREA\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     69\u001b[0m img_gray \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mcvtColor(resized_img, cv2\u001b[38;5;241m.\u001b[39mCOLOR_BGR2GRAY)\n\u001b[0;32m     70\u001b[0m scaledImg \u001b[38;5;241m=\u001b[39m img_gray\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m255.\u001b[39m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "try:\n",
    "    buffer_size = 1e4\n",
    "    batch_size = 32\n",
    "    episodes = 500\n",
    "    state_dim = (128, 128)\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    num_actions = len(action_values)\n",
    "    in_channels = 1\n",
    "    \n",
    "    replay_memory = ReplayMemory(state_dim, batch_size, buffer_size, device)\n",
    "    model = DQNAgent(num_actions, state_dim, in_channels, device)\n",
    "\n",
    "    # this only works if you have a model in your weights folder. Replace this by that file\n",
    "    model.load('../weights/model_ep_490')\n",
    "\n",
    "    # set to True if you want to run with pygame\n",
    "    env = CarEnv(visuals=False)\n",
    "\n",
    "    for ep in range(episodes):\n",
    "        env.step(model, replay_memory, ep, action_map)\n",
    "        env.reset()\n",
    "finally:\n",
    "    env.reset()\n",
    "    env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3629f74f-afd6-43f6-a68f-5701b9899626",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e112debd-100d-4094-a08c-e3d090eab923",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CARLA",
   "language": "python",
   "name": "carla"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
